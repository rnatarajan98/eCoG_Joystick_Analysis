{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3cceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda? False\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "\n",
    "# Install Packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy --quiet\n",
    "!{sys.executable} -m pip install matplotlib --quiet\n",
    "!{sys.executable} -m pip install seaborn --quiet\n",
    "!{sys.executable} -m pip install sklearn --quiet\n",
    "!{sys.executable} -m pip install torch --quiet\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import os, requests\n",
    "from matplotlib import rcParams \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data Retrieval\n",
    "\n",
    "fname = 'joystick_track.npz'\n",
    "url = \"https://osf.io/6jncm/download\"\n",
    "\n",
    "print('Is cuda?', torch.cuda.is_available())\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "# Import matplotlib and set styling\n",
    "rcParams['figure.figsize'] = [20, 4]\n",
    "rcParams['font.size'] =15\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "colourmap_diverge = sns.diverging_palette(321, 172, s=100, n=100, center = \"light\", as_cmap=True)\n",
    "colourmap = sns.color_palette(\"rocket\", as_cmap=True)\n",
    "colourmap = sns.light_palette(\"#30887c\", as_cmap=True)\n",
    "colourmap_diverge.set_bad(\"black\", alpha=0)\n",
    "colourmap.set_bad(\"black\", alpha=0)\n",
    "\n",
    "# Data Loading\n",
    "alldat = np.load(fname, allow_pickle=True)['dat']\n",
    "\n",
    "# Select just one of the recordings here. This is subject 1, block 1.\n",
    "dat = alldat[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace271ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalisation of voltage channels\n",
    "\n",
    "Vmax =  max((max(x) for x in dat['V']))\n",
    "Vmin = min((min(x) for x in dat['V']))\n",
    "xmax = max(dat['cursorX'])\n",
    "xmin = min(dat['cursorX'])\n",
    "ymax = max(dat['cursorY'])\n",
    "ymin = min(dat['cursorY'])\n",
    "\n",
    "def Vnormalise(_V):\n",
    "    _V_norm = (_V - Vmin)/(Vmax - Vmin)\n",
    "    return _V_norm\n",
    "\n",
    "def normalise(_V):\n",
    "    _V_norm = (_V - min(_V))/(max(_V) - min(_V))\n",
    "    return _V_norm\n",
    "\n",
    "def Xnormalise(_x):\n",
    "    _x_norm = (_x - xmin)/(xmax - xmin)\n",
    "    return _x_norm\n",
    "    \n",
    "def Xdenormalise(_x):\n",
    "    _x_denorm = _x*(xmax - xmin) + xmin\n",
    "    return _x_denorm\n",
    "\n",
    "def Ynormalise(_y):\n",
    "    _y_norm = (_y - ymin)/(ymax - ymin)\n",
    "    return _y_norm\n",
    "    \n",
    "def Ydenormalise(_y):\n",
    "    _y_denorm = _y*(ymax - ymin) + ymin\n",
    "    return _y_denorm\n",
    "\n",
    "  \n",
    "def downsample(signal, factor):\n",
    "  nbins = math.floor(nt/factor)\n",
    "  signal_norm = np.zeros((nbins, 1))\n",
    "  for ibin in range(nbins):\n",
    "    binstart = ibin * 40\n",
    "    binend = binstart + 40\n",
    "    signal_norm[ibin, 0] = np.mean(signal[binstart:binend])\n",
    "\n",
    "  return signal_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ba072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples 134360 number of channels 64\n",
      "V shape (134360, 64)\n",
      "cx, cy shape (134360, 1) (134360, 1)\n",
      "post normalisation:\n",
      "number of samples 134360 number of channels 64\n",
      "V shape (3359, 64)\n",
      "cx, cy shape (3359, 1) (3359, 1)\n"
     ]
    }
   ],
   "source": [
    "ds = True #downsample data?\n",
    "select_channels = False #Use correlated channels only?\n",
    "\n",
    "\n",
    "# Load patient 2\n",
    "dat = alldat[0, 2]\n",
    "corr_chan = sorted([5, 23, 17, 1, 25])\n",
    "\n",
    "V = dat['V']\n",
    "V = Vnormalise(V)\n",
    "nt, nchan = V.shape\n",
    "\n",
    "if select_channels:\n",
    "  V = V[:, corr_chan]\n",
    "\n",
    "cx = Xnormalise(dat['cursorX'].flatten()).reshape(-1,1)\n",
    "cy = Ynormalise(dat['cursorY'].flatten()).reshape(-1,1)\n",
    "\n",
    "print('number of samples', nt, 'number of channels', nchan)\n",
    "print('V shape', V.shape)\n",
    "print('cx, cy shape', cx.shape, cy.shape)\n",
    "\n",
    "factor = 40 #cursor trajectory updates every 40 samples, hardware limitation\n",
    "if downsample:\n",
    "  nbins = math.floor(nt/factor)\n",
    "\n",
    "  V_norm = np.zeros((nbins, nchan))\n",
    "  for c in range(nchan):\n",
    "    V_norm[:,c] = downsample(V[:,c], 40).flatten()\n",
    "  V = V_norm\n",
    "  cx = downsample(cx, 40)\n",
    "  cy = downsample(cy, 40)\n",
    "\n",
    "  print(\"post normalisation:\")\n",
    "  print('number of samples', nt, 'number of channels', nchan)\n",
    "  print('V shape', V.shape)\n",
    "  print('cx, cy shape', cx.shape, cy.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df464f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list() # instantiate X and y\n",
    "    for i in range(len(input_sequences)):\n",
    "        # find the end of the input, output sequence\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out - 1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(input_sequences): break\n",
    "        # gather input and output of the pattern\n",
    "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x), y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10e6cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3136, 200, 64) (3136, 25)\n",
      "Training Shape: (2687, 200, 64) (2687, 25)\n",
      "Testing Shape: (449, 200, 64) (449, 25)\n",
      "Test/Train cutoff 2687\n",
      "Total data points 3359\n"
     ]
    }
   ],
   "source": [
    "lookback = 200\n",
    "lookahead = 25\n",
    "X_ss, y_ss = split_sequences(V, cx, lookback, lookahead)\n",
    "print(X_ss.shape, y_ss.shape)\n",
    "\n",
    "total_samples = len(V)\n",
    "train_test_cutoff = round(0.80 * total_samples)\n",
    "\n",
    "X_train = X_ss[:train_test_cutoff]\n",
    "X_test = X_ss[train_test_cutoff:]\n",
    "\n",
    "y_train = y_ss[:train_test_cutoff]\n",
    "y_test = y_ss[train_test_cutoff:] \n",
    "\n",
    "print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape:\", X_test.shape, y_test.shape) \n",
    "print(\"Test/Train cutoff\", train_test_cutoff)\n",
    "print(\"Total data points\", total_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f840b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # output size\n",
    "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 100) # fully connected \n",
    "        self.fc_2 = nn.Linear(100, num_classes) # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # hidden state\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # cell state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
    "        hn = hn.view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) # first dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_2(out) # final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845c1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training\n",
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train,\n",
    "                  X_test, y_test):\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train) # forward pass\n",
    "        optimiser.zero_grad() # calculate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward() # calculates the loss of the loss function\n",
    "        optimiser.step() # improve from loss, i.e backprop\n",
    "        # test loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "        train_losses.append(loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: %d, train loss: %1.5f, test loss: %1.5f\" % (epoch, \n",
    "                                                                      loss.item(), \n",
    "                                                                      test_loss.item())) \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24dfe446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, set architecture\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_epochs = 1000 # 2000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = nchan # number of features\n",
    "hidden_size = 10 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "num_classes = lookahead # number of output classes \n",
    "\n",
    "lstm_x = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm_x.parameters(), lr=learning_rate) \n",
    "\n",
    "train_loss, test_loss = training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm_x,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=y_test_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting functions\n",
    "\n",
    "def plot_prediction(model, inputs, targets):\n",
    "\n",
    "  df_X_ss = inputs\n",
    "  df_y_mm = targets\n",
    "  # split the sequence\n",
    "  X_ss, y_ss = split_sequences(inputs, targets, lookback, lookahead)\n",
    "\n",
    "  # converting to tensors\n",
    "  df_X_ss = Variable(torch.Tensor(X_ss))\n",
    "  df_y_mm = Variable(torch.Tensor(y_ss))\n",
    "  # reshaping the dataset\n",
    "  df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], lookback, df_X_ss.shape[2]))\n",
    "\n",
    "  train_predict = model(df_X_ss) # forward pass\n",
    "  data_predict = train_predict.data.numpy() # numpy conversion\n",
    "  dataY_plot = df_y_mm.data.numpy()\n",
    "\n",
    "\n",
    "  true, preds = [], []\n",
    "  for i in range(len(dataY_plot)):\n",
    "      true.append(dataY_plot[i][0])\n",
    "  for i in range(len(data_predict)):\n",
    "      preds.append(data_predict[i][0])\n",
    "  plt.figure(figsize=(10,6)) #plotting\n",
    "  plt.axvline(x=train_test_cutoff, c='r', linestyle='--') # size of the training set\n",
    "\n",
    "  plt.plot(true, label='Actual Data') # actual plot\n",
    "  plt.plot(preds, label='Predicted Data') # predicted plot\n",
    "  plt.title('Time-Series Prediction')\n",
    "  plt.legend()\n",
    "  plt.savefig(\"whole_plot.png\", dpi=300)\n",
    "  plt.show() \n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(true, label='Actual Data') # actual plot\n",
    "  plt.plot(preds, label='Predicted Data') # predicted plot\n",
    "  plt.title('Time-Series Prediction')\n",
    "  plt.legend()\n",
    "  plt.savefig(\"whole_plot.png\", dpi=300)\n",
    "  plt.xlim(left = train_test_cutoff - train_test_cutoff*0/1)\n",
    "  plt.axvline(x=train_test_cutoff, c='r', linestyle='--') # size of the training set\n",
    "  plt.show() \n",
    "\n",
    "  print(\"test/train boundary:\", train_test_cutoff)\n",
    "  print(\"test/train proportion:\", train_test_cutoff/nt)\n",
    "  rms_test = rms(np.array(true), np.array(preds))\n",
    "  print(rms_test)\n",
    "\n",
    "\n",
    "def plot_losses(train_loss, test_loss):\n",
    "  plt.plot(train_loss, label=\"Train Losses\")\n",
    "  plt.plot(test_loss, label=\"Test Losses\")\n",
    "  plt.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9771c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global metric to compare models\n",
    "\n",
    "#Define common metric between models\n",
    "def rms(truth, prediction):\n",
    "    _diff = truth - prediction\n",
    "    _diff_flat = _diff.flatten()\n",
    "    _rms = np.sqrt(np.mean(_diff_flat**2))\n",
    "    return _rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b5c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model predictions\n",
    "\n",
    "plot_prediction(lstm_x, V, cx)\n",
    "plot_losses(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e79b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above for cursorY\n",
    "\n",
    "X_ss, y_ss = split_sequences(V, cy, lookback, lookahead)\n",
    "print(X_ss.shape, y_ss.shape)\n",
    "\n",
    "X_train = X_ss[:train_test_cutoff]\n",
    "X_test = X_ss[train_test_cutoff:]\n",
    "\n",
    "y_train = y_ss[:train_test_cutoff]\n",
    "y_test = y_ss[train_test_cutoff:] \n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
    "                                      (X_train_tensors.shape[0], lookback, \n",
    "                                       X_train_tensors.shape[2]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                     (X_test_tensors.shape[0], lookback, \n",
    "                                      X_test_tensors.shape[2])) \n",
    "\n",
    "#Defined as x model\n",
    "lstm_y = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm_y.parameters(), lr=learning_rate) \n",
    "\n",
    "train_loss, test_loss = training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm_y,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=y_test_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed55875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y coord "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
